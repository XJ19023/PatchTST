{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llama_codebook/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"/cephfs/juxin/models/opt-1.3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model.to(device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt = \"Q: What is the capital of China?\\nA:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Output ===\n",
      "Q: What is the capital of China?\n",
      "A: Beijing.  Source:\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== Output ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复制并添加模型的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model.test = copy.deepcopy(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resnet101, resnet50, resnet18\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'quant'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" ImageNet Training Script\n",
    "\n",
    "This is intended to be a lean and easily modifiable ImageNet training script that reproduces ImageNet\n",
    "training results with some of the latest networks and training techniques. It favours canonical PyTorch\n",
    "and standard Python style over trying to be able to 'do it all.' That said, it offers quite a few speed\n",
    "and training result improvements over the usual PyTorch example scripts. Repurpose as you see fit.\n",
    "\n",
    "This script was started from an early version of the PyTorch ImageNet example\n",
    "(https://github.com/pytorch/examples/tree/master/imagenet)\n",
    "\n",
    "NVIDIA CUDA specific speedups adopted from NVIDIA Apex examples\n",
    "(https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n",
    "\"\"\"\n",
    "import argparse\n",
    "import copy\n",
    "import os.path\n",
    "import random\n",
    "import socket\n",
    "from functools import partial\n",
    "\n",
    "import torch.distributed\n",
    "import torch.utils.data\n",
    "from timm.data.dataset import ImageDataset\n",
    "from timm.utils import accuracy\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from quant import *\n",
    "from utils import *\n",
    "from utils.resnet import resnet101, resnet50, resnet18\n",
    "from utils.vision_transformer import vit_small_patch16_224\n",
    "from utils.utils import write, create_transform, create_loader, AverageMeter, broadcast_tensor_from_main_process, \\\n",
    "    gather_tensor_from_multi_processes, compute_quantized_params\n",
    "\n",
    "HOST_NAME = socket.getfqdn(socket.gethostname())\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "LINEAR_COMPENSATION_SAMPLES = 512\n",
    "\n",
    "model_path = {\n",
    "    'resnet18': 'pretrained_weights/resnet18_imagenet.pth.tar',\n",
    "    'resnet50': 'pretrained_weights/resnet50_imagenet.pth.tar',\n",
    "    'resnet101': 'pretrained_weights/resnet101-63fe2227.pth'\n",
    "}\n",
    "\n",
    "def seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class CompensationBlock(nn.Module):\n",
    "    def __init__(self, W, b, r2_score, block, groups, linear_init=True, local_rank=0, block_id=None):\n",
    "        super(CompensationBlock, self).__init__()\n",
    "        self.block = block\n",
    "        self.groups = groups\n",
    "\n",
    "        self.lora_weight = nn.Parameter(torch.zeros((W.size(0), W.size(1), W.size(2), W.size(3))))\n",
    "        self.lora_bias = nn.Parameter(torch.zeros(b.size(0)))\n",
    "\n",
    "        if linear_init and (r2_score > 0):\n",
    "            self.lora_weight.data.copy_(W)\n",
    "            self.lora_bias.data.copy_(b)\n",
    "            if local_rank == 0:\n",
    "                _write('block {} using linear init'.format(block_id))\n",
    "        else:\n",
    "            nn.init.zeros_(self.lora_weight)\n",
    "            nn.init.zeros_(self.lora_bias)\n",
    "            if local_rank == 0:\n",
    "                _write('block {} using lora init'.format(block_id))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "\n",
    "        B, C_X, H_X, W_X = x.size()\n",
    "        _, C_Y, H_Y, W_Y = out.size()\n",
    "\n",
    "        if (H_X == H_Y) and (W_X == W_Y):\n",
    "            stride = 1\n",
    "        elif (H_X // 2 == H_Y) and (W_X // 2 == W_Y):\n",
    "            stride = 2\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.training:\n",
    "            qwt_out = F.conv2d(x, self.lora_weight, self.lora_bias, stride=stride, padding=int(self.lora_weight.size(-1) // 2), groups=self.groups)\n",
    "        else:\n",
    "            qwt_out = F.conv2d(x.half(), self.lora_weight.half(), None, stride=stride, padding=int(self.lora_weight.size(-1) // 2), groups=self.groups)\n",
    "            qwt_out = qwt_out.float() + self.lora_bias.reshape(1, -1, 1, 1)\n",
    "\n",
    "        out = out + qwt_out\n",
    "\n",
    "        return out\n",
    "\n",
    "def enable_quant(submodel):\n",
    "    for name, module in submodel.named_modules():\n",
    "        if isinstance(module, QuantConv2d) or isinstance(module, QuantLinear) or isinstance(module, QuantMatMul):\n",
    "            module.set_quant_state(True, True)\n",
    "\n",
    "def disable_quant(submodel):\n",
    "    for name, module in submodel.named_modules():\n",
    "        if isinstance(module, QuantConv2d) or isinstance(module, QuantLinear) or isinstance(module, QuantMatMul):\n",
    "            module.set_quant_state(False, False)\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item]\n",
    "\n",
    "def lienar_regression(X, Y, kernel_size=3, groups=4, block_id=0):\n",
    "    X = gather_tensor_from_multi_processes(X, args.world_size)\n",
    "    Y = gather_tensor_from_multi_processes(Y, args.world_size)\n",
    "\n",
    "    B, C_X, H_X, W_X = X.size()\n",
    "    _, C_Y, H_Y, W_Y = Y.size()\n",
    "\n",
    "    if (H_X == H_Y) and (W_X == W_Y):\n",
    "        stride = 1\n",
    "    elif (H_X // 2 == H_Y) and (W_X // 2 == W_Y):\n",
    "        stride = 2\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # calculate channles per group\n",
    "    C_per_group = C_X // groups\n",
    "    _C_per_group = C_Y // groups\n",
    "\n",
    "    # use Unfold to extract local patchs for each group\n",
    "    unfold = nn.Unfold(kernel_size=kernel_size, stride=stride, padding=int(kernel_size//2))\n",
    "\n",
    "    # process input and output in a group-wise manner\n",
    "    weights_list = []\n",
    "    bias_list = []\n",
    "    for g in range(groups):\n",
    "        # input channels for current group\n",
    "        X_group = X[:, g * C_per_group: (g + 1) * C_per_group, :, :]\n",
    "\n",
    "        # input patchs for current group\n",
    "        X_unfold_group = unfold(X_group)  # [B, C_per_group * kernel_size * kernel_size, L]\n",
    "        L = X_unfold_group.shape[-1]\n",
    "\n",
    "        # output channels for current group\n",
    "        Y_group = Y[:, g * _C_per_group: (g + 1) * _C_per_group, :, :]\n",
    "\n",
    "        # flatting Y\n",
    "        Y_flat_group = Y_group.view(B, _C_per_group, -1)  # [B, _C_per_group, L]\n",
    "\n",
    "        # concate all batchs to form an integrated equations\n",
    "        X_batch_all = X_unfold_group.permute(0, 2, 1).reshape(-1, C_per_group * kernel_size * kernel_size)  # [B*L, C_per_group * kernel_size * kernel_size]\n",
    "        Y_batch_all = Y_flat_group.permute(0, 2, 1).reshape(-1, _C_per_group)  # [B*L, _C_per_group]\n",
    "\n",
    "        # bias term\n",
    "        X_with_bias = torch.cat([X_batch_all, torch.ones(X_batch_all.shape[0], 1).cuda()], dim=1)  # [B*L, C_per_group * kernel_size * kernel_size + 1]\n",
    "\n",
    "        regularization = 1e-3\n",
    "        # add regularization term in case that XTX is inreversible\n",
    "        XTX = X_with_bias.T @ X_with_bias\n",
    "        XTX_reg = XTX + regularization * torch.eye(XTX.shape[0]).cuda()\n",
    "\n",
    "        # analytical solution for linear regression\n",
    "        W = torch.inverse(XTX_reg) @ X_with_bias.T @ Y_batch_all  # [C_per_group * kernel_size * kernel_size + 1, _C_per_group]\n",
    "\n",
    "        # decoule W and b\n",
    "        M_group = W[:-1, :].T\n",
    "        b_group = W[-1, :]\n",
    "\n",
    "        weights_list.append(M_group)\n",
    "        bias_list.append(b_group)\n",
    "\n",
    "    M_reshaped = torch.cat(weights_list, dim=0).view(C_Y, C_X // groups, kernel_size, kernel_size)\n",
    "\n",
    "    b_final = torch.cat(bias_list, dim=0)  # [_C]\n",
    "\n",
    "    W = M_reshaped\n",
    "    b = b_final\n",
    "\n",
    "    Y_pred = F.conv2d(X, W, b, stride=stride, padding=kernel_size//2, groups=groups)\n",
    "\n",
    "    abs_loss = (Y - Y_pred).abs().mean()\n",
    "\n",
    "    ss_tot = torch.sum((Y - Y.mean(dim=0)).pow(2))\n",
    "    ss_res = torch.sum((Y - Y_pred).pow(2))\n",
    "    r2_score = 1 - ss_res / ss_tot\n",
    "\n",
    "    _write('block : {}      abs : {:.6f}      r2 : {:.3f}'.format(block_id, abs_loss, r2_score))\n",
    "\n",
    "    return W, b, r2_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_compensation_model(q_model, train_loader, args):\n",
    "    _write('start to generate compensation model')\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    output_t = torch.zeros(size=[0,], device=args.device)\n",
    "    for i, (image, _) in tqdm(enumerate(train_loader)):\n",
    "        image = image.cuda()\n",
    "        t_out = q_model.forward_before_blocks(image)\n",
    "        output_t = torch.cat([output_t, t_out.detach()], dim=0)\n",
    "        torch.cuda.synchronize()\n",
    "        if i >= (LINEAR_COMPENSATION_SAMPLES // args.batch_size // args.world_size - 1):\n",
    "            break\n",
    "\n",
    "\n",
    "    feature_set = FeatureDataset(output_t.detach().cpu())\n",
    "    feature_loader = torch.utils.data.DataLoader(feature_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    output_previous = output_t\n",
    "    sup_layers = [q_model.layer1, q_model.layer2, q_model.layer3, q_model.layer4]\n",
    "    for sup_id in range(len(sup_layers)):\n",
    "        current_sup_layer = sup_layers[sup_id]\n",
    "        for layer_id in range(len(current_sup_layer)):\n",
    "\n",
    "            feature_set.X = output_previous.detach().cpu()\n",
    "\n",
    "            layer = current_sup_layer[layer_id]\n",
    "            output_full_precision = torch.zeros(size=[0, ], device=args.device)\n",
    "            output_quant = torch.zeros(size=[0, ], device=args.device)\n",
    "            output_t_ = torch.zeros(size=[0, ], device=args.device)\n",
    "            for i, t_out in tqdm(enumerate(feature_loader)):\n",
    "                t_out = t_out.cuda()\n",
    "                disable_quant(layer)\n",
    "                full_precision_out = layer(t_out)\n",
    "\n",
    "                enable_quant(layer)\n",
    "                quant_out = layer(t_out)\n",
    "\n",
    "                output_t_ = torch.cat([output_t_, t_out.detach()], dim=0)\n",
    "                output_full_precision = torch.cat([output_full_precision, full_precision_out.detach()], dim=0)\n",
    "                output_quant = torch.cat([output_quant, quant_out.detach()], dim=0)\n",
    "\n",
    "                torch.cuda.synchronize()\n",
    "                if i >= (LINEAR_COMPENSATION_SAMPLES // args.batch_size  // args.world_size - 1):\n",
    "                    break\n",
    "\n",
    "            assert torch.sum((output_previous - output_t_).abs()) < 1e-3\n",
    "            global_layer_id = sum(q_model.depths[:sup_id]) + layer_id\n",
    "            W, b, r2_score = lienar_regression(output_t_, output_full_precision - output_quant, kernel_size=args.kernel_size, groups=max(output_t_.size(1) // args.factor, 1), block_id=global_layer_id)\n",
    "            current_sup_layer[layer_id] = CompensationBlock(W=W, b=b, r2_score=r2_score, block=current_sup_layer[layer_id], groups=max(output_t_.size(1) // args.factor, 1), linear_init=True if (global_layer_id >= args.start_block) else False, local_rank=args.local_rank, block_id=global_layer_id)\n",
    "            q_model.cuda()\n",
    "\n",
    "            qwerty_layer = current_sup_layer[layer_id]\n",
    "\n",
    "            output_previous = torch.zeros(size=[0, ], device=args.device)\n",
    "            for i, t_out in tqdm(enumerate(feature_loader)):\n",
    "                t_out = t_out.cuda()\n",
    "                enable_quant(qwerty_layer)\n",
    "                previous_out = qwerty_layer(t_out)\n",
    "\n",
    "                output_previous = torch.cat([output_previous, previous_out.detach()], dim=0)\n",
    "\n",
    "                torch.cuda.synchronize()\n",
    "                if i >= (LINEAR_COMPENSATION_SAMPLES // args.batch_size // args.world_size - 1):\n",
    "                    break\n",
    "\n",
    "    return q_model\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\", default=\"resnet18\", choices=['resnet18', 'resnet50', 'resnet101'], help=\"model\")\n",
    "parser.add_argument('--data_dir', default='../ImageNet', type=str)\n",
    "\n",
    "parser.add_argument('--w_bits', default=4, type=int, help='bit-precision of weights')\n",
    "parser.add_argument('--a_bits', default=4, type=int, help='bit-precision of activations')\n",
    "parser.add_argument('--start_block', default=0, type=int)\n",
    "parser.add_argument('--kernel_size', default=1, type=int)\n",
    "parser.add_argument('--factor', default=64, type=int)\n",
    "\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"batchsize of validation set\")\n",
    "parser.add_argument('--num_workers', default=4, type=int)\n",
    "parser.add_argument(\"--seed\", default=0, type=int, help=\"seed\")\n",
    "\n",
    "parser.add_argument(\"--local-rank\", default=0, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "train_aug = 'large_scale_train'\n",
    "test_aug = 'large_scale_test'\n",
    "args.drop_path = 0.0\n",
    "args.num_classes = 1000\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "crop_pct = 0.875\n",
    "\n",
    "args.distributed = False\n",
    "if 'WORLD_SIZE' in os.environ:\n",
    "    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n",
    "args.device = 'cuda:0'\n",
    "args.world_size = 1\n",
    "args.rank = 0  # global rank\n",
    "if args.distributed:\n",
    "    args.device = 'cuda:%d' % args.local_rank\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    args.world_size = torch.distributed.get_world_size()\n",
    "    args.rank = torch.distributed.get_rank()\n",
    "\n",
    "assert args.rank >= 0\n",
    "\n",
    "\n",
    "args.log_dir = os.path.join('checkpoint', args.model, 'QwTGroupConv', 'bs_{}_worldsize_{}_w_{}_a_{}_kernelsize_{}_factor_{}_startblock_{}_sed_{}' .format(args.batch_size, args.world_size, args.w_bits, args.a_bits, args.kernel_size, args.factor, args.start_block, args.seed))\n",
    "\n",
    "args.log_file = os.path.join(args.log_dir, 'log.txt')\n",
    "\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    if not os.path.exists(args.log_dir):\n",
    "        os.makedirs(args.log_dir)\n",
    "\n",
    "    if os.path.isfile(args.log_file):\n",
    "        os.remove(args.log_file)\n",
    "else:\n",
    "    time.sleep(1)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "_write = partial(write, log_file=args.log_file)\n",
    "\n",
    "if args.distributed:\n",
    "    _write('Training in distributed mode with multiple processes, 1 GPU per process. Process %d, total %d.' % (args.rank, args.world_size))\n",
    "else:\n",
    "    _write('Training with a single process on 1 GPUs.')\n",
    "assert args.rank >= 0\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        _write(args)\n",
    "\n",
    "    seed(args.seed)\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        _write('dataset mean : {} & std : {}'.format(mean, std))\n",
    "\n",
    "    dataset_train = ImageDataset(root=os.path.join(args.data_dir, 'train'), transform=create_transform(train_aug, mean, std, crop_pct))\n",
    "    dataset_eval = ImageDataset(root=os.path.join(args.data_dir, 'val'), transform=create_transform(test_aug, mean, std, crop_pct))\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        _write('len of train_set : {}    train_transform : {}'.format(len(dataset_train), dataset_train.transform))\n",
    "        _write('len of eval_set : {}    eval_transform : {}'.format(len(dataset_eval), dataset_eval.transform))\n",
    "\n",
    "\n",
    "    loader_train = create_loader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        is_training=True,\n",
    "        re_prob=0.0,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        num_workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        log_file=args.log_file,\n",
    "        drop_last=True,\n",
    "        local_rank=args.local_rank,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "    loader_eval = create_loader(\n",
    "        dataset_eval,\n",
    "        batch_size=args.batch_size,\n",
    "        is_training=False,\n",
    "        re_prob=0.,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        num_workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        log_file=args.log_file,\n",
    "        drop_last=False,\n",
    "        local_rank=args.local_rank,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "    for data, _ in loader_train:\n",
    "        calib_data = data.to(args.device)\n",
    "        break\n",
    "\n",
    "    broadcast_tensor_from_main_process(calib_data, args)\n",
    "    _write('local_rank : {} calib_data shape : {} value : {}'.format(args.local_rank, calib_data.size(), calib_data[0, 0, 0, :5]))\n",
    "\n",
    "\n",
    "\n",
    "    _write('Building model ...')\n",
    "    if args.model == 'resnet18':\n",
    "        model = resnet18(num_classes=args.num_classes, pretrained=False)\n",
    "    elif args.model == 'resnet50':\n",
    "        model = resnet50(num_classes=args.num_classes, pretrained=False)\n",
    "    elif args.model == 'resnet101':\n",
    "        model = resnet101(num_classes=args.num_classes, pretrained=False)\n",
    "    elif args.model == 'vit-s':\n",
    "        model = vit_small_patch16_224(num_classes=args.num_classes, pretrained=False)\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
